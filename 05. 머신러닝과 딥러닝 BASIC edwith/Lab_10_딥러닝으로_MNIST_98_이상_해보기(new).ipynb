{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 10: 딥러닝으로 MNIST 98%이상 해보기(new).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3w49GD0fx1i",
        "outputId": "0a3b0bc9-917e-44ae-92cd-25a895730796"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5M4mrKpg6MmI",
        "outputId": "76fb3ea8-5279-44e0-ce54-0f39a1128104"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "# 0 - 9 digits recognition = 10 classes\n",
        "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
        "b = tf.Variable(tf.random_normal([nb_classes]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training cycle\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-98a9ad1d53d3>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Epoch: 0001, Cost: 2.826302671\n",
            "Epoch: 0002, Cost: 1.061668953\n",
            "Epoch: 0003, Cost: 0.838061300\n",
            "Epoch: 0004, Cost: 0.733232729\n",
            "Epoch: 0005, Cost: 0.669279874\n",
            "Epoch: 0006, Cost: 0.624611825\n",
            "Epoch: 0007, Cost: 0.591160346\n",
            "Epoch: 0008, Cost: 0.563868980\n",
            "Epoch: 0009, Cost: 0.541745169\n",
            "Epoch: 0010, Cost: 0.522673575\n",
            "Epoch: 0011, Cost: 0.506782328\n",
            "Epoch: 0012, Cost: 0.492447637\n",
            "Epoch: 0013, Cost: 0.479955836\n",
            "Epoch: 0014, Cost: 0.468893668\n",
            "Epoch: 0015, Cost: 0.458703486\n",
            "Learning finished\n",
            "Accuracy:  0.8951\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe6G5Dd86pzj"
      },
      "source": [
        "# NN for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C-WNLLL6cAu",
        "outputId": "8903cbe7-91cc-4c45-b5f8-1281e8568216"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-3-ab46dcd085e5>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 154.764451219\n",
            "Epoch: 0002 cost = 38.226437567\n",
            "Epoch: 0003 cost = 24.175796071\n",
            "Epoch: 0004 cost = 16.529306580\n",
            "Epoch: 0005 cost = 11.986585262\n",
            "Epoch: 0006 cost = 8.827067470\n",
            "Epoch: 0007 cost = 6.500990301\n",
            "Epoch: 0008 cost = 4.850937089\n",
            "Epoch: 0009 cost = 3.651409691\n",
            "Epoch: 0010 cost = 2.778389789\n",
            "Epoch: 0011 cost = 2.157732659\n",
            "Epoch: 0012 cost = 1.647687733\n",
            "Epoch: 0013 cost = 1.360324939\n",
            "Epoch: 0014 cost = 0.981299468\n",
            "Epoch: 0015 cost = 0.781419999\n",
            "Learning Finished!\n",
            "Accuracy: 0.9459\n",
            "Label:  [1]\n",
            "Prediction:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKD-1jza7Gwm"
      },
      "source": [
        "# 초기화를 잘 해야 한다.\n",
        "\n",
        "XAVIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RvoEI446cGz",
        "outputId": "e1c398ba-4ccb-4619-c67b-900c1d346a3c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.get_variable(\"W1\",shape=[784, 256],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\",shape=[256, 256],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\",shape=[256, 10],\n",
        "                    initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost = 0.297378644\n",
            "Epoch: 0002 cost = 0.113608453\n",
            "Epoch: 0003 cost = 0.074200059\n",
            "Epoch: 0004 cost = 0.052476243\n",
            "Epoch: 0005 cost = 0.038890316\n",
            "Epoch: 0006 cost = 0.030232352\n",
            "Epoch: 0007 cost = 0.025023081\n",
            "Epoch: 0008 cost = 0.020546650\n",
            "Epoch: 0009 cost = 0.015858955\n",
            "Epoch: 0010 cost = 0.014852626\n",
            "Epoch: 0011 cost = 0.013901817\n",
            "Epoch: 0012 cost = 0.010400721\n",
            "Epoch: 0013 cost = 0.010093268\n",
            "Epoch: 0014 cost = 0.010284236\n",
            "Epoch: 0015 cost = 0.009722549\n",
            "Learning Finished!\n",
            "Accuracy: 0.9777\n",
            "Label:  [2]\n",
            "Prediction:  [2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh5zYltX73qe"
      },
      "source": [
        "# Deep for MNIST\n",
        "\n",
        "overfiting 가능성이 있다. \n",
        "새로운 데이터이면 확도가 떨어진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc348bBa6cJ9",
        "outputId": "a1c3fd24-8308-4521-eb29-a59d5dcf785f"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-03bd18a79b68>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-3-03bd18a79b68>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 0.296339564\n",
            "Epoch: 0002 cost = 0.105193180\n",
            "Epoch: 0003 cost = 0.071178786\n",
            "Epoch: 0004 cost = 0.052508223\n",
            "Epoch: 0005 cost = 0.039383502\n",
            "Epoch: 0006 cost = 0.036201993\n",
            "Epoch: 0007 cost = 0.031511294\n",
            "Epoch: 0008 cost = 0.026299873\n",
            "Epoch: 0009 cost = 0.021789706\n",
            "Epoch: 0010 cost = 0.020645806\n",
            "Epoch: 0011 cost = 0.020267152\n",
            "Epoch: 0012 cost = 0.019223929\n",
            "Epoch: 0013 cost = 0.016839738\n",
            "Epoch: 0014 cost = 0.015505837\n",
            "Epoch: 0015 cost = 0.015411342\n",
            "Learning Finished!\n",
            "Accuracy: 0.9784\n",
            "Label:  [2]\n",
            "Prediction:  [2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsQ9K3kr8RG3",
        "outputId": "65857baf-1381-43a9-989a-a20f553ce24b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# weights & bias for nn layers\n",
        "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
        "\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys,keep_prob:0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels,keep_prob:1}))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-c9124f3ce882>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-c9124f3ce882>:30: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-2-c9124f3ce882>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 0.468772872\n",
            "Epoch: 0002 cost = 0.169264645\n",
            "Epoch: 0003 cost = 0.128351285\n",
            "Epoch: 0004 cost = 0.104887483\n",
            "Epoch: 0005 cost = 0.090467301\n",
            "Epoch: 0006 cost = 0.083585275\n",
            "Epoch: 0007 cost = 0.072925550\n",
            "Epoch: 0008 cost = 0.067548715\n",
            "Epoch: 0009 cost = 0.062108426\n",
            "Epoch: 0010 cost = 0.059084329\n",
            "Epoch: 0011 cost = 0.056056181\n",
            "Epoch: 0012 cost = 0.054252551\n",
            "Epoch: 0013 cost = 0.049097619\n",
            "Epoch: 0014 cost = 0.047702543\n",
            "Epoch: 0015 cost = 0.046401804\n",
            "Learning Finished!\n",
            "Accuracy: 0.9817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7CpLi2H9mB5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}